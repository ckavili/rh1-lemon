# First run these commands
# oc new-project lemonade-stand
# oc create secret generic hf-secret --from-literal=HF_TOKEN=<your_huggingface_token> -
---
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: openshift-ai-inference
spec:
  controllerName: openshift.io/gateway-controller/v1
---
# GatewayConfig for resource limits on the gateway pod
apiVersion: v1
kind: ConfigMap
metadata:
  name: openshift-ai-inference-config
  namespace: openshift-ingress
data:
  resources: |
    requests:
      cpu: "2"
      memory: "4Gi"
    limits:
      cpu: "4"
      memory: "8Gi"
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  labels:
    istio.io/rev: openshift-gateway
  name: openshift-ai-inference
  namespace: openshift-ingress
spec:
  gatewayClassName: openshift-ai-inference
  listeners:
    - allowedRoutes:
        namespaces:
          from: Selector
          selector:
            matchExpressions:
              - key: kubernetes.io/metadata.name
                operator: In
                values:
                  - lemonade-stand
      hostname: inference-gateway.apps.lemon.rhoai.rh-aiservices-bu.com # needs to be updated with the actual ingress domain
      name: https
      port: 443
      protocol: HTTPS
      tls:
        certificateRefs:
          - group: ''
            kind: Secret
            name: default-gateway-tls
        mode: Terminate
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  annotations:
    opendatahub.io/model-type: generative
    openshift.io/display-name: llama-llm-d
    security.opendatahub.io/enable-auth: 'false'
  name: llama-llm-d
  namespace: lemonade-stand
  finalizers:
    - serving.kserve.io/llmisvc-finalizer
spec:
  model:
    name: llama32
    uri: oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct
  replicas: 48 # change replicas as needed
  router:
    gateway: {}
    route: {}
    scheduler:
      template:
        containers:
          - name: main
            resources:
              limits:
                cpu: '2'
                memory: 24Gi
              requests:
                cpu: '1'
                memory: 16Gi
  template:
    containers:
      - env:
          - name: VLLM_ADDITIONAL_ARGS
            value: '--model=/mnt/models --served-model-name=llama32 --disable-uvicorn-access-log --max-model-len=512 --gpu-memory-utilization=0.91 --max-num-batched-tokens=32768 --max-num-seqs=512 --num-scheduler-steps=8 --enable-prefix-caching --enable-chunked-prefill'
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 30
        name: main
        resources:
          limits:
            cpu: '2'
            memory: 24Gi
            nvidia.com/gpu: '1'
          requests:
            cpu: '1'
            memory: 16Gi
            nvidia.com/gpu: '1'
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        value: NVIDIA-H200-PRIVATE

